name: BTC Sentiment Pipeline

on:
  # Run on schedule (every hour)
  schedule:
    - cron: '0 * * * *'  # Every hour at minute 0
  
  # Allow manual triggering
  workflow_dispatch:
  
  # Run on push to main (optional - for testing)
  push:
    branches:
      - main
    paths:
      - 'src/pipelines/**'
      - '.github/workflows/pipeline.yml'

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Check disk space
        run: |
          df -h
          echo "Available disk space before installation"
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install CPU-only PyTorch first (much smaller, ~200MB vs ~3GB)
          # This prevents running out of disk space on GitHub Actions
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          # Install transformers (depends on torch)
          pip install transformers
          # Then install rest of requirements (excluding torch and transformers)
          pip install fastapi uvicorn streamlit sqlalchemy pandas pydantic-settings plotly requests pytest httpx python-json-logger feedparser schedule
          # Clean up pip cache to save space
          pip cache purge
      
      - name: Check disk space after installation
        run: |
          df -h
          echo "Available disk space after installation"
      
      - name: Create .env file
        run: |
          # Create .env from GitHub Secrets
          # Use secrets if available, otherwise use defaults
          cat > .env << EOF
          DB_URL=sqlite:///btc_sentiment.db
          NEWS_FEEDS=${{ secrets.NEWS_FEEDS }}
          REDDIT_FEEDS=${{ secrets.REDDIT_FEEDS }}
          COINGECKO_BASE=${{ secrets.COINGECKO_BASE }}
          ALLOWED_ORIGINS=${{ secrets.ALLOWED_ORIGINS }}
          EOF
          # Replace empty values with defaults
          python3 << 'PYTHON'
          import os
          with open('.env', 'r') as f:
              content = f.read()
          # Set defaults for empty values
          lines = content.split('\n')
          result = []
          for line in lines:
              if line.startswith('NEWS_FEEDS=') and line == 'NEWS_FEEDS=':
                  result.append('NEWS_FEEDS=https://cointelegraph.com/rss')
              elif line.startswith('REDDIT_FEEDS=') and line == 'REDDIT_FEEDS=':
                  result.append('REDDIT_FEEDS=bitcoin')
              elif line.startswith('COINGECKO_BASE=') and line == 'COINGECKO_BASE=':
                  result.append('COINGECKO_BASE=https://api.coingecko.com/api/v3')
              elif line.startswith('ALLOWED_ORIGINS=') and line == 'ALLOWED_ORIGINS=':
                  result.append('ALLOWED_ORIGINS=http://localhost:8501,http://localhost:8000')
              else:
                  result.append(line)
          with open('.env', 'w') as f:
              f.write('\n'.join(result))
          print("Created .env file:")
          with open('.env', 'r') as f:
              print(f.read())
          PYTHON
      
      - name: Download database (if exists)
        id: download-db
        uses: actions/download-artifact@v4
        with:
          name: database
          path: .
        continue-on-error: true
      
      - name: Check if database was downloaded
        run: |
          if [ -f "btc_sentiment.db" ]; then
            echo "âœ… Found existing database, will append new data"
            ls -lh btc_sentiment.db
          else
            echo "â„¹ï¸ No existing database found - this is normal on first run"
            echo "Creating new database..."
          fi
      
      - name: Run collection pipeline
        run: |
          python -m src.pipelines.collect
      
      - name: Run scoring pipeline
        run: |
          python -m src.pipelines.score
      
      - name: Run aggregation pipeline (hourly)
        run: |
          python -m src.pipelines.aggregate --granularity hourly --days 7
      
      - name: Run aggregation pipeline (daily)
        run: |
          python -m src.pipelines.aggregate --granularity daily --days 30
      
      - name: Cleanup old data (keep 60 days)
        run: |
          python -m src.pipelines.cleanup --retention-days 60
      
      - name: Upload database as artifact
        if: always()  # Upload even if some steps failed
        uses: actions/upload-artifact@v4
        with:
          name: database
          path: btc_sentiment.db
          retention-days: 7
          if-no-files-found: warn  # Warn instead of failing if file doesn't exist
      
      - name: Summary
        run: |
          echo "âœ… Pipeline completed successfully"
          if [ -f "btc_sentiment.db" ]; then
            echo "ðŸ“¦ Database saved as artifact for next run"
            ls -lh btc_sentiment.db
          else
            echo "âš ï¸ No database file found to upload"
          fi

